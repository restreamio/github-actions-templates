---
name: Build and deploy to CloudFront

concurrency:
  group: ${{ inputs.concurrency_name }}
  cancel-in-progress: true

on:
  workflow_call:
    inputs:
      # =====
      # Setup
      # =====
      instance_type:
        type: string
        default: xlarge
        required: false
      node_version:
        type: string
        # If set to '' version will be extracted from .nvmrc file
        default: "16"
        required: false
      node_options:
        type: string
        required: false
      package_manager:
        type: string
        default: yarn
        required: false
      # =====
      # Build
      # =====
      build_command:
        type: string
        default: yarn build
        required: false
      build_cache_path:
        type: string
        required: false
      build_path:
        type: string
        default: ./dist
        required: false
      # ===========
      # Source maps
      # ===========
      upload_source_maps:
        type: boolean
        default: false
        required: false
      # If not specified, will use package.json version
      sentry_release_version:
        type: string
        required: false
      sentry_org:
        type: string
        default: restream-io
        required: false
      # If not specified, Sentry step won't run
      sentry_project:
        type: string
        required: false
      datadog_release_version:
        type: string
        required: false
      # ======
      # Deploy
      # ======
      s3_bucket_name:
        type: string
        required: true
      invalidate_cloudfront_cache:
        type: boolean
        default: true
        required: false
      # ======
      # NX monorepo tools
      # ======
      with_nx:
        type: boolean
        default: false
        required: false
      concurrency_name:
        type: string
        default: "build-n-deploy-to-cf"
        required: false

    secrets:
      sentry_auth_token:
        required: false
      datadog_api_key:
        required: false

jobs:
  build:
    name: Build and deploy to CloudFront
    runs-on: ["self-hosted", "${{ inputs.instance_type }}", "prod"]
    steps:
      - name: Show context
        env:
          # To make debugging easier in case of issues
          GITHUB_CONTEXT_JSON: ${{ toJson(github) }}
          INPUTS_JSON: ${{ toJson(inputs) }}
        run: |
          echo Github context: "$GITHUB_CONTEXT_JSON"
          echo Inputs: "$INPUTS_JSON"

      - name: Checkout source code
        uses: actions/checkout@v4
        timeout-minutes: 1
        with:
          fetch-depth: 0

      - name: Setup Node
        uses: actions/setup-node@v4
        timeout-minutes: 5
        with:
          node-version: ${{ inputs.node_version }}
          node-version-file: .nvmrc
          cache: ${{ inputs.package_manager }}

      - name: Get package version
        # Version needed only for source maps, so disable it if project doesn't post them to Sentry or DataDog
        if: inputs.upload_source_maps
        id: "get_package_version"
        run: |
          echo Package version: "$(npm pkg get version --workspaces=false | sed 's/"//g')"
          echo "package_version=$(npm pkg get version --workspaces=false | sed 's/"//g')" >> $GITHUB_OUTPUT

      - name: Install dependencies
        run: ${{ (inputs.package_manager == 'yarn') && 'npm install -g yarn && yarn install --frozen-lockfile' || 'npm ci' }}
        timeout-minutes: 10

      - name: Cache build
        if: inputs.build_cache_path
        timeout-minutes: 5
        uses: actions/cache@v4
        with:
          path: ${{ inputs.build_cache_path }}
          key: build-cache-${{ (inputs.package_manager == 'yarn') && hashFiles('yarn.lock') || hashFiles('package-lock.json') }}
          restore-keys: |
            build-cache-${{ (inputs.package_manager == 'yarn') && hashFiles('yarn.lock') || hashFiles('package-lock.json') }}
            build-cache-

      - uses: nrwl/nx-set-shas@v3
        with:
          main-branch-name: "master"
        if: inputs.with_nx

      - run: git branch --track master origin/master
        if: ${{ github.event_name == 'pull_request' && inputs.with_nx }}

      - name: Build
        run: ${{ inputs.build_command }}
        timeout-minutes: 15
        env:
          COMMIT_HASH: ${{ github.sha }}
          NODE_OPTIONS: ${{ inputs.node_options }}

      - name: Upload source maps to Sentry
        if: inputs.upload_source_maps && inputs.sentry_project
        uses: getsentry/action-release@v1.7.0
        env:
          SENTRY_AUTH_TOKEN: ${{ secrets.sentry_auth_token }}
          SENTRY_ORG: ${{ inputs.sentry_org }}
          SENTRY_PROJECT: ${{ inputs.sentry_project }}
        timeout-minutes: 5
        with:
          environment: ${{ (github.ref == 'refs/heads/master') && 'production' || 'staging' }}
          # We do not use Github integration, hence it will always fail
          set_commits: skip
          version: ${{ inputs.sentry_release_version && inputs.sentry_release_version || steps.get_package_version.outputs.package_version }}

      - name: Upload source maps to DataDog
        if: inputs.upload_source_maps
        run: |
          npx @datadog/datadog-ci sourcemaps upload ${{ inputs.build_path }} \
            --service=${{ github.event.repository.name }} \
            --release-version=${{ inputs.datadog_release_version && inputs.datadog_release_version || steps.get_package_version.outputs.package_version }} \
            --minified-path-prefix=/
        timeout-minutes: 5
        env:
          DATADOG_API_KEY: ${{ secrets.datadog_api_key }}

      - name: Deploy to CloudFront
        if: github.ref == 'refs/heads/master'
        timeout-minutes: 15
        run: |
          cd ${{ inputs.build_path }}
          aws s3 cp . "s3://${{ inputs.s3_bucket_name }}" --recursive --no-progress --exclude "*.js.map"

      - name: Invalidate CloudFront cache
        if: ${{ (inputs.invalidate_cloudfront_cache) && (github.ref == 'refs/heads/master') }}
        timeout-minutes: 5
        env:
          ORIGIN: ${{ inputs.s3_bucket_name }}.s3.amazonaws.com
        run: |
          echo "DISTRIBUTIONS=$(aws cloudfront list-distributions \
            --query "DistributionList.Items[*].{id:Id,origin:Origins.Items[0].Id}[?origin=='${{ env.ORIGIN }}'].id" \
            --output text \
          )" >> $GITHUB_ENV

          for id in $DISTRIBUTIONS; do
            aws cloudfront create-invalidation --distribution-id $id --paths "/*"
          done

  cleanup:
    name: Cleanup
    runs-on: ["self-hosted", "${{ inputs.instance_type }}", "prod"]
    if: (github.event_name == 'pull_request' && github.event.action == 'closed') || (github.event_name == 'pull_request' && github.event.pull_request.merged == true)
    steps:
      - name: Clean Actions caches
        run: |
          python3 -c '
          import json
          import urllib.request
          import urllib.parse
          import os

          def make_request(url, method="GET", data=None, headers=None):
              if headers is None:
                  headers = {}

              default_headers = {
                  "Accept": "application/vnd.github+json",
                  "Authorization": "Bearer ${{ secrets.GITHUB_TOKEN }}",
                  "X-GitHub-Api-Version": "2022-11-28"
              }
              headers = {**default_headers, **headers}

              try:
                  if data:
                      data = json.dumps(data).encode("utf-8")

                  req = urllib.request.Request(url, data=data, headers=headers, method=method)
                  with urllib.request.urlopen(req) as response:
                      if method == "GET":
                          return json.loads(response.read().decode("utf-8"))
                      return True
              except urllib.error.HTTPError as e:
                  print(f"HTTP Error: {e.code} - {e.reason}")
                  print(e.read().decode("utf-8"))
                  raise

          def get_branch_caches(owner, repo, ref, page=1):
              params = {
                  "ref": ref,
                  "per_page": "100",
                  "page": str(page),
                  "sort": "last_accessed_at",
                  "direction": "desc"
              }
              query_string = urllib.parse.urlencode(params)
              url = f"https://api.github.com/repos/{owner}/{repo}/actions/caches?{query_string}"
              return make_request(url)

          def delete_cache(owner, repo, cache_id):
              url = f"https://api.github.com/repos/{owner}/{repo}/actions/caches/{cache_id}"
              return make_request(url, method="DELETE")

          # get env vars
          github_repository = os.environ["GITHUB_REPOSITORY"]
          owner, repo = github_repository.split("/")

          ref = "refs/pull/${{ github.event.pull_request.number }}/merge"
          print(f"Cleaning caches for ref: {ref}")

          # get caches for the branch
          response = get_branch_caches(owner, repo, ref)
          caches = response.get("actions_caches", [])

          print(f"Found {len(caches)} caches")

          # delete up to 100 caches
          deleted_count = 0
          for cache in caches[:100]:
              cache_id = cache["id"]
              print(f"Deleting cache: {cache_id}")
              try:
                  if delete_cache(owner, repo, cache_id):
                      print(f"Successfully deleted cache: {cache_id}")
                      deleted_count += 1
              except Exception as e:
                  print(f"Failed to delete cache {cache_id}: {str(e)}")
          '
